# metaphor_detector

## 运行

1. 由于文件大小的限制。 需要自行下载数据集`flickr30k`和`MET-Meme`数据集、`word2vec`。
2. `calculate_disambiguator_input.py`：用 CLIP 编码`flickr30k`的图像和文本，用`word2vec`编码文本，作为消歧器的输入。
3. `run_disambiguator_train.py`：训练消歧器
4. `calculate_classifier_input.py`:用训练得到的消歧器和 CLIP 编码`MET-Meme`英文的图像和文本，获得分类器的输入。
5. `run_classifier_train.py`:训练分类器，获得最终模型
6. `flickr30k`需要自行划分训练集与测试集:(

## 目前最佳

- 测试集上： acc: `90.62` f1_pos: `80.92` f1_neg: `93.79` f1_avg: `90.17` (pos:225, neg:575)
- 在`Contrast`分支上达到 acc: `91.50` f1_avg: `91.08`

## 说明与结论

- 由于`CLIP`不支持中文，实验仅在`MET-Meme`的英文数据集上进行
- `gpt-2`必要性不强，不如说白增加了很多参数，`Contrast`分支上仅使用一些全连接网络就取得了与之相近的结果
- `w2v`效果也并不强，`Contrast`分支上将`w2v`的向量全替换成零向量，效果稍有提升。`Contrast`似乎比MLP效果强一些，不知道为什么。
- `Contrast`分支中最后判断隐喻的方式是基于一个分数`distance`，当它大于`0.3`时判断为隐喻。在下游使用神经网络反而没有这个阈值有效。感觉很怪，好像又不是不可以(?)
- 尝试过一些创飞了的方法，没有发上来：
  - 1. 将`CLIP`的第一层和最后一层拿出来，拼接，过一些前馈网络，然后在序列上做最大池化。有一定效果，但是比`Contrast`和`gpt-2`效果都差一些，不过参数量非常少(
  - 2. 从`CLIP`中均匀抽取一些层，组成一个大张量，用卷积神经网络处理这个大张量。基本没什么效果，不能排除是我网络设计有问题。
  - 3. 上面两个方法灵感来源是一个直觉“`CLIP`浅层的表示应该比深层表示更能反映表层含义，可以用来对比”。
